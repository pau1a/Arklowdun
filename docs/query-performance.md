# events_list_range query latency benchmark

This document tracks baseline performance for the `time query-bench` harness that measures `events_list_range` latency across representative calendar windows (day, week, month). The goal is a **repeatable reference** for detecting regressions as the calendar command surface evolves.

---

## Fixture dataset

The benchmark uses deterministic SQLite fixtures generated by `scripts/bench/generate_query_fixture.ts`. The generator seeds events that mix timed sessions, all-day blocks, recurring series, and UTC-series with EXDATEs using a fixed Mulberry32 seed so repeated runs produce identical data unless parameters change.

```ts
const DEFAULT_ROWS = 10_000;
const DEFAULT_SEED = 104729; // prime seed for deterministic spread
// ...
const rand = mulberry32(opts.seed);
````

Generate (examples):

```sh
# 10k baseline dataset
node --loader ts-node/esm scripts/bench/generate_query_fixture.ts --rows 10000

# Light 1k dataset for CI smoke
node --loader ts-node/esm scripts/bench/generate_query_fixture.ts --rows 1000
```

Fixtures are written to:

* `fixtures/time/query/query-10k.sqlite3`
* `fixtures/time/query/query-1k.sqlite3`

---

## Harness usage

The `time` maintenance binary exposes a `query-bench` subcommand that:

* Loads a fixture into a temporary DB,
* Randomises daily start offsets,
* Executes `events_list_range` across requested windows,
* Aggregates latency (min, P50, P95, max, mean), item counts, truncation frequency, and sampled date ranges,
* Emits human-readable output **and** a final JSON summary (1 line) for tooling.

Run (example):

```sh
# macOS example from repo root:
(cd src-tauri && cargo run --locked --bin time -- \
  query-bench --rows 10000 --iterations 50 --warmup 5 \
  --window day --window week --window month)
```

---

## Environment

**Local baseline (Mac)**

| Field        | Value                                                             |
| ------------ | ----------------------------------------------------------------- |
| Date (local) | 2025-09-21                                                        |
| Machine      | MacBook Pro (Apple Silicon / modern Intel OK)                     |
| OS           | macOS (local dev environment)                                     |
| Notes        | Measurements taken after warm-ups; final line is JSON for parsing |

*(A separate Linux/Codespaces profile can be captured in future; this doc records the local Mac baseline used to guard regressions in day-to-day development.)*

---

## Baseline results (10k events, macOS local)

Command:

```sh
# Generate fixture (repo root)
node --loader ts-node/esm scripts/bench/generate_query_fixture.ts --rows 10000
# Run benchmark (src-tauri)
cargo run --locked --bin time -- query-bench --rows 10000 --iterations 50 --warmup 5
```

Summary (milliseconds; items are the mean number of rows returned):

| Window | Duration | Min | P50 | P95 | Max | Mean | Avg items | Truncated |
| ------ | -------- | --: | --: | --: | --: | ---: | --------: | --------: |
| Day    | 1 day    |  54 |  71 | 102 | 105 |    — |       292 |         0 |
| Week   | 7 days   |  57 |  82 | 107 | 113 |    — |     2 213 |         0 |
| Month  | 30 days  |  66 |  96 | 124 | 127 |    — |     8 931 |        35 |

Notes:

* The month window deliberately stresses recurrence expansion; truncation is expected under the global cap and is correctly reported.
* “Mean” latency is available in the JSON but omitted from the table for brevity; use the JSON for precise diffing.

**Extracting a paste-ready table from the JSON line**

```sh
tail -n1 /tmp/query10k.json \
| jq -r '.windows | (["Window","Min","P50","P95","Max","Avg_items","Truncated"]), (.[] | [ .window, (.min_ms|floor), (.p50_ms|floor), (.p95_ms|floor), (.max_ms|floor), (.items_mean|floor), .truncated ]) | @tsv'
```

---

## Repeatability check (same seed)

Run the same command twice with the same `--seed` and compare P50/P95 per window. Variance within ±10% is acceptable for the local baseline.

```sh
# Example pattern (capture two runs and diff P50/P95 via jq/python)
```

*(See PR-17 notes for exact one-liners used during validation.)*

---

## CI smoke test (1k dataset)

The smoke verifies the harness and invariants quickly without gating performance:

```sh
# Generate a 1k fixture
npm run bench:query:gen -- --rows 1000

# Run a short benchmark (from src-tauri)
cargo run --locked --bin time -- query-bench --rows 1000 --iterations 10 --warmup 2
```

**Machine-checked invariants (jq on the JSON line):**

* Latency ordering per window:

  ```sh
  tail -n1 <<<"$(cargo run --locked --bin time -- query-bench --rows 1000 --iterations 10 --warmup 2)" \
  | jq -e 'all(.windows[]; .min_ms <= .p50_ms and .p50_ms <= .p95_ms and .p95_ms <= .max_ms)'
  ```
* Item scaling by window (day ≤ week ≤ month):

  ```sh
  tail -n1 <<<"$(cargo run --locked --bin time -- query-bench --rows 1000 --iterations 10 --warmup 2)" \
  | jq -e '([.windows[] | {(.window): .items_mean}] | add) as $m | ($m.day <= $m.week and $m.week <= $m.month)'
  ```

*(Wire these into a lightweight GitHub Action so every PR exercises the harness and catches structural regressions.)*

---

## Notes & usage tips

* The JSON summary is **only the last line** of the CLI output; pipe `tail -n1` into `jq` for parsing.
* To keep a working copy of the database used during a run (for ad-hoc inspection), add `--keep-db`.
* To benchmark a specific fixture file, pass `--fixture <path>`; otherwise the harness resolves a default path based on `--rows`.
